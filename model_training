# ─── Full Training Pipeline with TQDM, Early Stopping & Checkpoints ───
import os, time
import torch
from torch.utils.data import DataLoader, random_split
from torch.optim import AdamW
from transformers import (
    Blip2Processor,
    Blip2ForConditionalGeneration,
    get_scheduler
)
from datasets import load_dataset
from PIL import Image
from peft import get_peft_model, LoraConfig, TaskType
from tqdm.auto import tqdm
import matplotlib.pyplot as plt

# 1) Config
MODEL_NAME     = "Salesforce/blip2-opt-2.7b"
MANIFEST       = "blip2_manifest.csv"
OUTPUT_DIR     = "blip2-lora-opt2.7b-checkpoints"
os.makedirs(OUTPUT_DIR, exist_ok=True)

NUM_EPOCHS     = 3
PATIENCE       = 2
TRAIN_FRACT    = 0.8
BATCH_SIZE     = 1
LR             = 3e-4
LORA_RANK      = 8
MAX_LEN        = 128

# 2) Load & preprocess entire CSV
ds = load_dataset("csv", data_files=MANIFEST, split="train")
processor = Blip2Processor.from_pretrained(MODEL_NAME)
base_model = Blip2ForConditionalGeneration.from_pretrained(MODEL_NAME)

# 3) Freeze vision tower
for p in base_model.vision_model.parameters():
    p.requires_grad = False

# 4) Wrap with LoRA
lora_cfg = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    inference_mode=False,
    r=LORA_RANK,
    lora_alpha=32,
    lora_dropout=0.05
)
model = get_peft_model(base_model, lora_cfg)

# 5) Preprocessing helper
def preprocess_fn(ex):
    img = Image.open(ex["image_png"]).convert("RGB")
    enc = processor(
        images=img,
        text=ex["report"],
        padding="max_length",
        truncation=True,
        max_length=MAX_LEN,
        return_tensors="pt",
    )
    enc["labels"] = enc["input_ids"]
    return {k: v.squeeze(0) for k, v in enc.items()}

# 6) Map & format
processed = ds.map(preprocess_fn, batched=False, remove_columns=ds.column_names)
processed.set_format(
    type="torch",
    columns=["pixel_values","input_ids","attention_mask","labels"]
)

# 7) Split into train/val
total = len(processed)
train_size = int(TRAIN_FRACT * total)
val_size   = total - train_size
train_ds, val_ds = random_split(processed, [train_size, val_size])

# 8) DataLoaders
train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)

# 9) Prepare optimizer, scheduler, device
device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
model.to(device)
optimizer = AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=LR
)
total_steps = len(train_loader) * NUM_EPOCHS
scheduler   = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

# 10) Training loop with early stopping & tqdm
best_val = float("inf")
no_improve = 0
train_losses, val_losses = [], []

for epoch in range(1, NUM_EPOCHS+1):
    print(f"\n--- Epoch {epoch}/{NUM_EPOCHS} ---")
    start = time.time()

    # Training
    model.train()
    running_train = 0.0
    train_bar = tqdm(train_loader, desc="  Training", leave=False)
    for batch in train_bar:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(
            pixel_values=batch["pixel_values"],
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            labels=batch["labels"],
        )
        loss = outputs.loss
        running_train += loss.item()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()

        train_bar.set_postfix(loss=f"{loss.item():.3f}")

    avg_train = running_train / len(train_loader)
    train_losses.append(avg_train)

    # Validation
    model.eval()
    running_val = 0.0
    val_bar = tqdm(val_loader, desc="  Validation", leave=False)
    with torch.no_grad():
        for batch in val_bar:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(
                pixel_values=batch["pixel_values"],
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"],
                labels=batch["labels"],
            )
            running_val += outputs.loss.item()
            val_bar.set_postfix(val_loss=f"{outputs.loss.item():.3f}")

    avg_val = running_val / len(val_loader)
    val_losses.append(avg_val)

    # Summary + checkpointing
    elapsed = time.time() - start
    print(f"Epoch {epoch} done in {elapsed:.1f}s → "
          f"train_loss={avg_train:.4f}, val_loss={avg_val:.4f}")

    if avg_val < best_val:
        best_val = avg_val
        no_improve = 0
        ckpt_dir = os.path.join(OUTPUT_DIR, f"epoch{epoch}-val{avg_val:.4f}")
        model.save_pretrained(ckpt_dir)
        print(f"  ▶ Saved best model to {ckpt_dir}")
    else:
        no_improve += 1
        if no_improve >= PATIENCE:
            print("Early stopping triggered.")
            break

# 11) Plot loss curves
plt.plot(range(1, len(train_losses)+1), train_losses, label="Train")
plt.plot(range(1, len(val_losses)+1),   val_losses,   label="Val")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training & Validation Loss")
plt.legend()
plt.show()
